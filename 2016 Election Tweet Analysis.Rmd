---
title: "2016 Election Tweets"
author: "Mike"
date: "2/16/2020"
output: 
  html_document:
    toc: TRUE
    keep_md: TRUE


---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}

library(tidyverse)
library(caret)
library(igraph)
library(readr)
library(knitr)
library(lubridate)
library(gridExtra)
library(reshape2)
library(topicmodels)
library(gutenbergr)
library(tm)
library(SnowballC)
library(ldatuning)
library(tidytext)
library(wordcloud)
library(corrplot)
library(viridis)
library(viridisLite)

# !diagnostics off

tweets <- read_csv("Data/Twitter.csv")
users <- read_csv("Data/User.csv")

data <- tweets


# Date and Data Parsing --------------------------

users$month<-sapply(users$created_at, function(x) match(strsplit(x," ")[[1]][2], month.abb))
users$year<-sapply(users$created_at, function(x) as.numeric(strsplit(x," ")[[1]][6]))
users$day<-sapply(users$created_at, function(x) as.numeric(strsplit(x," ")[[1]][3]))
users$DayTS<-as.Date(paste0(users$year,'-',users$month,'-',users$day), format="%Y-%m-%d")
#clean from empty creation date
#users<-data.frame(users %>% filter(created_at !=""))

tweets$DayTS<-as.Date(tweets$created_str,format="%Y-%m-%d")
tweets$year<-year(tweets$DayTS)
tweets$month<-month(tweets$DayTS)
tweets$day<-day(tweets$DayTS)
tweets$weekdays<-weekdays(tweets$DayTS)
tweets$week<-week(tweets$DayTS)
tweets$weekdays <- factor(tweets$weekdays, levels = rev(c("Monday", "Tuesday", "Wednesday", "Thursday","Friday", "Saturday", "Sunday")))

users$date <- as.POSIXct(users$DayTS)


time <- as.POSIXct(strptime(c("2009-01-07 00:00:00","2017-09-26 23:59:59"), format = "%Y-%m-%d %H:%M:%S"))

# Graphing ------------------
# Plot by Year-Month
# https://stackoverflow.com/questions/17758006/time-series-plot-with-x-axis-in-year-month-in-r

time <- as.POSIXct(strptime(c("2009-01-07 00:00:00","2017-09-26 23:59:59"), format = "%Y-%m-%d %H:%M:%S"))

user.creation <- users %>% group_by(year,month) %>% 
                    summarise(count=n()) %>% 
                    mutate(dateTS = as.Date(paste0(year,'-',month,'-01'),format="%Y-%m-%d")) %>% 
                    ggplot(aes(x=as.POSIXct(dateTS),y=count)) + 
                    geom_point(size=1) + 
                    geom_line(alpha=.5,size=1, color = "#71CA97") + 
                    scale_x_datetime(limits =time) + 
                    labs(title="Users Created by Month") + 
                    xlab("Month and Year") + 
                    ylab("Number of Users Created") +
                    theme_classic()


tweet.time <- tweets %>% group_by(year,month) %>% 
                         summarise(count=n()) %>% 
                         mutate(dateTS = as.Date(paste0(year,'-',month,'-01'),format="%Y-%m-%d")) %>% 
                         ggplot(aes(x=as.POSIXct(dateTS),y=count)) + geom_point(size=1) + 
                         geom_line(alpha=.5,size=1, color = "#71CA97") + 
                         scale_x_datetime(limits =time) + 
                         labs(title="Number of Tweets by Month") + 
                         xlab("Month and Year") + 
                         ylab("Number of Tweets") +
                         theme_classic()



pe <- tweets[(tweets$DayTS> "2016-01-01" & tweets$DayTS < "2016-12-31"),]
petime <- as.POSIXct(strptime(c("2016-01-01 00:00:00","2016-12-31 23:59:59"), format = "%Y-%m-%d %H:%M:%S"))

pe.month <- pe %>% group_by(year,month) %>% 
                   summarise(count=n()) %>% mutate(dateTS = as.Date(paste0(year,'-',month,'-01'),format="%Y-%m-%d")) %>% 
                   ggplot(aes(x=as.POSIXct(dateTS),y=count)) + 
                   geom_point(size=1) + 
                   geom_line(alpha=.5,size=1, color = "#71CA97") + 
                   scale_x_datetime(limits =petime) + 
                   labs(title="Tweets in 2016 by Month") + 
                   xlab("") + 
                   ylab("Number of Tweets") +
                   geom_vline(xintercept=as.numeric(as.POSIXct(as.Date('2016-11-08'))),color='blue') +
                   ggplot2::annotate("text", x=as.POSIXct(as.Date("2016-10-11")), y = 20000, label = "Election Day", size=3, colour="blue") +
                   theme_classic()



pe.day <- pe %>% group_by(DayTS) %>% summarise(count=n()) %>%
   ggplot(aes(x=DayTS,y=count)) + 
   geom_point(size=1) + 
   geom_line(alpha=.5,size=1, color = "#71CA97") + 
   labs(title="Tweets in 2016 by Day") + 
   xlab("") + 
   ylab("Number of Tweets") +
   geom_vline(xintercept=as.Date('2016-11-08'),color='blue') +
   ggplot2::annotate("text", x=as.Date("2016-11-08"), y = 4000, label = "Election Day", size=3, colour="blue") +
   geom_vline(xintercept=as.Date('2016-10-19'),color='red') +
   ggplot2::annotate("text", x=as.Date("2016-10-19"), y = 3500, label = "Access Hollywood", size=3, colour="red") +
   geom_vline(xintercept=as.Date('2016-10-07'),color='black') +
   ggplot2::annotate("text", x=as.Date("2016-10-07"), y = 3000, label = "Wikileaks emails \n release", size=3, colour="black") +
   geom_vline(xintercept=as.Date('2016-07-21'),color='red') +
   ggplot2::annotate("text", x=as.Date("2016-07-21"), y = 2000, label = "Trump Nomination", size=3, colour="red") +
   geom_vline(xintercept=as.Date('2016-03-22'),color='black') +
   ggplot2::annotate("text", x=as.Date("2016-03-22"), y = 1500, label = "Additional Primaries", size=3, colour="black") +
   geom_vline(xintercept=as.Date('2016-02-02'),color='red') +
   ggplot2::annotate("text", x=as.Date("2016-02-02"), y = 1000, label = "Iowa Caucuses", size=3, colour="red") +
   theme_classic()
  
  
```

## EDA

Exploring the data we see that troll users were created as far back as 2014.

```{r, echo=FALSE, warning=FALSE}

plot(user.creation)

```

A general graph of tweets over time shows a crescendo of tweets leading upto the November 2016 election.

```{r, echo=FALSE, warning=FALSE}

plot(tweet.time)
plot(pe.month)

```

Overlaying of other significant events during the 2016 election and the number of tweets sent by trolls. While this analysis will focus specifically on election day, additional analysis should be able to reflect the messaging related major news stories or events which (presumably) triggered the spike in activity. 

```{r, echo=FALSE, warning=FALSE}

plot(pe.day)  

```

To identify users from retweeted text, a short function parses mentioned users into an edgelist of the sending and receiving user. This list is compared to the dataset of known trolls to differentiate troll and non-troll users. Degree centrality, Betweeness, and Eigen centrality metrics are then calculated from the resulting edgelist. (Closeness centrality was omitted given the highly disconnected nature of the resulting network).

```{r, include=FALSE}

f <- subset(tweets, DayTS == "2016-11-08")


#pre.f <- subset(tweets, DayTS <= "2016-11-08")

z <- tweets



plotfunction <- function(df) {
  
  # RT Parsing --------------------------------------
  
  # Grep RT's 
  rt <- grep("^rt @[a-z0-9_]{1,15}", tolower(df$text), perl=T, value=T)
  
  # Select RT senders 
  rt.send <- tolower(as.character(df$user_key[grep("^rt @[a-z0-9_]{1,15}", tolower(df$text), perl=T)]))
  rt.rec <- tolower(regmatches(rt, regexpr("@(?U).*:", rt)))
  # Remove @ and :
  rt.rec <- (gsub(":", "", rt.rec))
  rt.rec <- (gsub("@", "", rt.rec)) 
  View(rt.rec)
  
  # Missing Values as NA
  rt.send[rt.send==""] <- "<NA>"
  rt.rec[rt.rec==""] <- "<NA>"
  
  # Create single df with all users
  users.all <- unique(as.data.frame(c(rt.send, rt.rec))) 
  #renaming the handle names variable
  users.all <- users.all %>% rename(user = "c(rt.send, rt.rec)")
  
  #Force global df
  rt.send <<- rt.send
  rt.rec <<- rt.rec
  
  
  df <- df %>% rename(user = user_key) #renaming user name variable
  tweets.user <- df %>% select(user) #selecting only the users from the data
  trolls <- users %>% select(screen_name)
  trolls <- trolls %>% rename(user = screen_name)
  
  trolls <- rbind(trolls, tweets.user)
  
  trolls.u <- unique(trolls) #removing duplicates
  trolls.u$troll <- "troll" #assigning all of these users a trolls
  ### matching trolls with the complete set of handle names in the retweet network
  
  nodes <- right_join(trolls.u, users.all)
  nodes <<- replace(nodes, is.na(nodes), "non-troll") 
  
  
  # Network graph creation ###########################
  
  # This is an edge list, who RTs who and how many times
  rt.df <<- data.frame(rt.send, rt.rec)
  ### creating the retweetnetwork based on the sender-receiver df and the node attributes (troll/non-troll)
  rt.g <<- graph.data.frame(rt.df, directed=T, vertices = nodes)
  
  
  # bipartite.mapping(rt.g)
  # V(rt.g)$type <- bipartite_mapping(rt.g)$type
  # plot(rt.g)
  
  ### removing self-ties
  rt.g.noloop <<-simplify(rt.g, remove.loops = T, remove.multiple = F)
  
  
  
  # Now we can compute basic centrality scores for each user and store it in a data frame.
  # removing multiple edges between users
  g <- simplify(rt.g.noloop, remove.multiple = T, remove.loops = T)
  # creating a data frame with weighted and unweighted degree centrality for each profile
  g.centrality <- data.frame(name =V(g)$name,
                             troll= V(g)$troll,indegree=degree(g,mode='in'),
                             indegree_weighted = degree(rt.g.noloop, mode ="in"),
                             outdegree=degree(g,mode='out'),
                             outdegree_weighted = degree(rt.g.noloop, mode = "out"))
  
  colnames(g.centrality)[colnames(g.centrality)=="name"] <- "user"
  g.centrality <<- g.centrality
  
  return(rt.g.noloop)
}




plotfunction(f)
# data frame f -------------------


f.cent.df <- nodes
f.cent.rt.g <- rt.g.noloop
f.cent.df$deg <- degree(rt.g.noloop)
f.cent.df$bet <- betweenness(rt.g.noloop)
f.cent.df$eig <- eigen_centrality(rt.g.noloop)$vector
f.cent.df <- f.cent.df %>% left_join(g.centrality[,c("user","indegree","indegree_weighted","outdegree","outdegree_weighted")], by=c("user"))

#graph from edge list
links <- rt.df
nodes.l <- f.cent.df

network.edge <- graph_from_data_frame(d=links, vertices = nodes.l, directed=T) 

# customGreen0 = "#DeF7E9"
# customGreen = "#71CA97"
# customRed = "#ff7f7f"
# customYel = "#C9C271"

V(network.edge)$color <- ifelse(V(network.edge)$troll == "troll", "#ff4d4d", "#71CA97")

# Make the plot
# Label size according to betweeness centrality
# Node / vertex size related to degree centrality
# Color

```

## Network Graph

Using a Fruchterman Reingold layout, the edge list is graphed. Node size is related to degree centrality, a simple popularity metric counting the number of "edges" running through the node. Node label are sized according to betweeness centrality, a measure of information brokerage, and idenfy the users who control the flow of information across nodes (Only three such users are identified here). Node and label color reflects troll (red) or non-troll (green) users. Here we can see the general structure, with trolls disemminating information out to groups of unlinked non-troll users. This supports what we have come to know about the election interference campaign, that trolls created messages targeted at specific subsets of users. 

```{r, fig.height=14, fig.width=14, echo=FALSE, warning=FALSE}

set.seed(1434)
plot(network.edge, 
     layout=layout.fruchterman.reingold,
     
     #Vertex/node
     vertex.size=((nodes.l$deg)/6),
     vertex.color=V(network.edge)$color,
     
     # Node label
     vertex.label.cex=(((nodes.l$bet)/9)+0.3),
     vertex.label.color=V(network.edge)$color,
     
     # Edge
     edge.width=.2,                      
     edge.arrow.size=0.02,                      
     edge.arrow.width=0.02,
     edge.curved=0.3,
     edge.color="gray"
     
)

```
```{r RT Percent LM, include=FALSE}

troll.z.cent.df <- f.cent.df %>% filter(troll == "troll")
z.cent.df <- f.cent.df
# Identfiy the top n% of trolls with the highest % change in degree --------------------------------------
#n <- 5
#top.trolls.delta.one <- subset(troll.z.cent.df, Delta1_deg > quantile(Delta1_deg, prob = 1 - n/100,na.rm=TRUE))
#top.trolls.delta.two <- subset(troll.z.cent.df, Delta2_deg > quantile(Delta2_deg, prob = 1 - n/100,na.rm=TRUE))

# Users: followers to status = gaining more followers with less tweets (more effective?)
# Number of tweets in the dataset will be different from the overall counts in the user data
# So outside fo followers to status we'll use numbers from the tweet dataset itself for any calculations
#users$followers_to_status <- (users$followers_count / users$statuses_count)

# Total and Average tweets per user within the dataset, ignore NA values
tweet.calc.sub <- subset(tweets, select = c(user_key, retweet_count, favorite_count))#, weekdays, week,DayTS, created_str))
tweet.calc.sub$NumberofTweetsInDataSet <- 1 
tweet.calc.sub <- tweet.calc.sub %>%
  group_by(user_key) %>%
  summarise_all(funs(mean(., na.rm = TRUE),sum(., na.rm = TRUE)))
# remove this column that makes no sense
tweet.calc.sub$NumberofTweetsInDataSet_mean <- NULL
# Replace error NaN (divide by zero) columns with NA
tweet.calc.sub$retweet_count_mean[is.nan(as.numeric(tweet.calc.sub$retweet_count_mean))] <- NA
tweet.calc.sub$favorite_count_mean[is.nan(as.numeric(tweet.calc.sub$favorite_count_mean))] <- NA

# adding calculations back to z dataframe
colnames(tweet.calc.sub)[colnames(tweet.calc.sub)=="user_key"] <- "user"
z.cent.df <- z.cent.df %>% left_join(tweet.calc.sub[,c("user","retweet_count_mean","favorite_count_mean","NumberofTweetsInDataSet_sum")], by=c("user"))

# Just renaming the troll.z.cent.df df for easy of memory going forward
troll.subset <- troll.z.cent.df %>% left_join(tweet.calc.sub[,c("user","retweet_count_mean","favorite_count_mean","NumberofTweetsInDataSet_sum")], by=c("user"))

#pulling the troll with the highest degree in the complete dataset
#ameliebaldwin <- tweets %>% filter(user_key == "ameliebaldwin")

# Finding Retweeting in dataset 
# Parsing RT from tweet (looking at first two letters only) 
#Using the z dataframe, a copy of tweets, to reduce touching original df
z$RT <- substr(tweets$text, start = 1, stop = 2)
#Numerical value for RT = true
z$RT <- (ifelse(z$RT == 'RT',"1","0"))
# Updating column name of z dataframe from user key to user for joining
#colnames(z)[colnames(z)=="user"] <- "user_key"
colnames(z)[colnames(z)=="user_key"] <- "user"
# Change to numeric data type for summarization
z$RT <- as.numeric(z$RT)
#Sum total RT's found by user, store in an intermediate table
RTtotal <- aggregate( RT ~ user, z, sum)
#Joining to the tweet calc subset for calculation
tweet.calc.sub <- tweet.calc.sub %>% left_join(RTtotal[,c("user","RT")], by=c("user"))

# calculating percent RT from total number of tweets in tweets/z df
tweet.calc.sub$percentRT <- (tweet.calc.sub$RT / tweet.calc.sub$NumberofTweetsInDataSet_sum) 
# Moving this number to the troll.z.cent.df with other measurements and metrics to be used in the model
troll.subset <- troll.subset %>% left_join(tweet.calc.sub[,c("user","percentRT","RT")], by=c("user"))


troll.bet.v.rt <- ggplot(troll.subset, aes(x=percentRT, y=bet)) + 
  geom_point() + 
  geom_smooth(method=lm , color="red", fill="#69b3a2", se=TRUE) +
  scale_y_log10()

troll.deg.v.rt <- ggplot(troll.subset, aes(x=percentRT, y=deg)) + 
  geom_point() + 
  geom_smooth(method=lm , color="red", fill="#69b3a2", se=TRUE) +
  scale_y_log10()

troll.eig.v.rt <- ggplot(troll.subset, aes(x=percentRT, y=eig)) + 
  geom_point() + 
  geom_smooth(method=lm , color="red", fill="#69b3a2", se=TRUE) +
  scale_y_log10() 


```

```{r, fig.height=10, fig.width=14, echo=FALSE, warning=FALSE,include=FALSE}

grid.arrange(troll.bet.v.rt,troll.deg.v.rt,troll.eig.v.rt,ncol=1,nrow=3)

```


```{r Model Performance, include=FALSE}

# Saving off the troll subset to another dataframe
troll.subset.save <- troll.subset

# Previous calculations left Nan and Inf values, will replace those with NA below:
# function to remove all nan values from troll subset 
is.nan.data.frame <- function(x)
  do.call(cbind, lapply(x, is.nan))
#Replace NaN values with NA
troll.subset[is.nan(troll.subset)] <- NA
#Replace inf values with NA
troll.subset <- do.call(data.frame,lapply(troll.subset, function(x) replace(x, is.infinite(x),NA)))


# Only taking the factors I really care about along for the modeling ride
troll.subset.limited <- troll.subset[,c("deg","eig","bet","retweet_count_mean","favorite_count_mean","percentRT")]


numericVars <- which(sapply(troll.subset.limited, is.numeric)) #index vector numeric variables
numericVarNames <- names(numericVars) #saving names vector for use later on
#cat('There are', length(numericVars), 'numeric variables')

# put numeric variables into the numeric data frame and calculate their correlations
all_numVar <- troll.subset.limited[, numericVars]
cor_numVar <- cor(all_numVar, use="pairwise.complete.obs") #correlations of all numeric variables

#sort on decreasing correlations 
cor_sorted <- as.matrix(sort(cor_numVar[,'percentRT'], decreasing = TRUE))
# #select only high corelations 
# Skip this, cause none of our variables correlate well... :-(
# CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.5)))
# cor_numVar <- cor_numVar[CorHigh, CorHigh]
# 
# corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt")


#pdf("8_correlation_plot_numeric_variables.pdf", width = 6, height = 5)
corrplot(cor_numVar)
#dev.off()

#check and remove zero or near zero variance features 
nzv <- nearZeroVar(troll.subset.limited, saveMetrics = TRUE)
# There are none
#troll.subset.limited.nzv <- troll.subset.limited[,-nzv]

## Create correlation table to find highly correlated features. Remove those with > 0.75 correlation
## to prevent multicollinearity
# troll.subset.limited.cor <- cor(troll.subset.limited)
# summary(troll.subset.limited.cor[upper.tri(troll.subset.limited.cor)])
# glmdnc <- findCorrelation(troll.subset.limited.cor, cutoff = .75)
# troll.subset.limited.cor.nocor <- troll.subset.limited.cor[,-glmdnc]
# 
# # Center and Scale values
# glm.data.nzv.nocor.hv <- subset(glm.data.nzv.nocor, select = c(Highvalue))
# glm.data.nzv.nocor <- subset(glm.data.nzv.nocor, select = -c(Highvalue))


# Preprocess values by centering and scaling, remove any NA values
set.seed(333)
troll.subset.limited.preprocess <- preProcess(troll.subset.limited, method = c("center","scale"))
set.seed(333)
glm.preprocess <- predict(troll.subset.limited.preprocess,troll.subset.limited)
glm.preprocess <- na.omit(glm.preprocess)


# Create random 60/40 training and testing dataset
set.seed(333)
smp_size <- floor(0.70 * nrow(glm.preprocess))
set.seed(333)
train_ind <- sample(seq_len(nrow(glm.preprocess)), size = smp_size)
glm.train <- glm.preprocess[train_ind,]
glm.test <- glm.preprocess[-train_ind,]


# Below, testing several different models to find the best accuracy

# glm <- glm(percentRT~., data = glm.train, family = "gaussian")
# 
# sink("9_glm_summary.txt")
# print(summary(glm))
# sink()
# 
# confint(glm)
# predict(glm, type="response")


# Setting control and performance metric RMSE
control <- trainControl(method ="cv", number = 10)
metric <- "RMSE"

# Train KNN, SVM, RF models for performance comparison
set.seed(333)
fit.knn <- train(percentRT~., data = glm.train, method="knn", metric=metric, trControl=control)
set.seed(333)
fit.svm <- train(percentRT~., data = glm.train, method="svmRadial", metric=metric, trControl=control)
set.seed(333)
fit.rf <- train(percentRT~., data = glm.train, method="rf", metric=metric, trControl=control, importance=T)
set.seed(333)
fit.glm <- train(percentRT~., data = glm.train, method="glm", metric=metric, trControl=control)
results <- resamples(list(knn=fit.knn, svm=fit.svm,rf=fit.rf,glm=fit.glm))

#summary(results)
#sink("10_model_results_summary.txt")
#print(summary(results))
#sink()

# Plot feature importances 
#scales <- list(x=list(relation="free"), y=list(relation="free"))

#pdf("11_model_importance_plot.pdf", width = 6, height = 3)
#dotplot(results, scales=scales)
#dev.off()


#pdf("11_RF_variable_importance_plot.pdf", width = 6, height = 3)
#plot(varImp(fit.rf, scale = FALSE))
#dev.off()

#rf.prediction <- predict(fit.rf, glm.test)
#print(postResample(pred = rf.prediction, obs = glm.test$percentRT))

```


```{r echo=FALSE, fig.height=10, fig.width=14, warning=FALSE, include=FALSE}

scales <- list(x=list(relation="free"), y=list(relation="free"))
dotplot(results, scales=scales)
plot(varImp(fit.rf, scale = FALSE))


```

## Text Analysis

Text analysis was performed using a small function to preprocess tweet text including: stemming, stop words removed, puncuation removed, conversion to lower case, numbers removed, white space removed. Additional words removed include "rt","amp","http". 

```{r Text Preprocessing, include=FALSE}
# Begin Text Analysis ---------------------------

#text <- tweets$text

textfunction <- function(text) {
  
  # Preprocessing to remove puncuation, upper case, numbers, white space, and stop words
  # https://compsocialscience.github.io/summer-institute/2018/materials/day3-text-analysis/basic-text-analysis/rmarkdown/Basic_Text_Analysis_in_R.html#tokenization
  # https://www.springboard.com/blog/text-mining-in-r/
  text <- as.data.frame(text)
  text <- data.frame(text %>% mutate(text = iconv(text, from = "latin1", to = "ASCII")) %>% filter(!is.na(text)))
  
  text.corp <- VCorpus(VectorSource(as.vector(text)))
  text.corp <- tm_map(text.corp, removePunctuation)
  text.corp <- tm_map(text.corp, content_transformer(tolower))
  text.corp <- tm_map(text.corp, removeNumbers)
  text.corp <- tm_map(text.corp, stripWhitespace)
  text.corp <- tm_map(text.corp, removeWords, stopwords("english"))
  
  # Removing additional words RT (retweet), amp (google amp mobile webpages?)
  # Untested code
  text.corp  <- tm_map(text.corp , removeWords, c(tidytext::stop_words$word,"rt","amp","http"))
  
  # Stemming
  text.corp <- tm_map(text.corp, stemDocument, language = "english")
  
  # Creat document term matrix and remove 0 word entries from the table
  text.dtm <- DocumentTermMatrix(text.corp, control = list(wordLengths = c(2, Inf)))
  
  # Another pass at taking out the trash then pushing it global for additional analysis
  text.corp <<- text.corp[!grepl('^http|amp|rt',text.corp)]
  text.corp.tdm <- TermDocumentMatrix(Corpus(VectorSource(text.corp)))
  text.corp.tdm.m <<- as.matrix(text.corp.tdm)
  
  # #Find the sum of words in each Document This method results in a vector of 370Gb, use method below
  # rowTotals <- apply(text.dtm, 1, sum) 
  # #remove all docs without words
  # text.dtm.one   <- text.dtm[rowTotals> 0, ]
  
  # A document-term-matrix created by the tm package contains the names i and j , 
  # which are indices for where entries are in the sparse matrix. 
  # If text.dtm$i does not contain a particular row index p, then row p is empty
  # ui contains all the non-zero indices, and since text.dtm$i is already ordered, text.dtm.n will be in the same order as text.dtm
  ui = unique(text.dtm$i)
  text.dtm.n = text.dtm[ui,]
  
  
  #Pushing non-zero document term matrixto global frame
  text.dtm.n <<- text.dtm.n
  
  return (text.dtm.n)
}





#f <- subset(tweets, DayTS == "2016-11-08")
f.text <- f$text
textfunction(f.text)
f.text.corp <- text.corp
f.text.dtm.n <- text.dtm.n
f.text.tdm.m <- text.corp.tdm.m

```

Natural language processing using Latent Dirichlet allocation, using k=2 for simplicity, the top 10 terms for each topic are plotted. 

```{r LDA, include=FALSE}

ff.text.lda <- LDA(f.text.dtm.n, k = 2, control = list(seed = 333))
ff.text.topics <- tidy(ff.text.lda, matrix = "beta")

ff.text.topics.top <- ff.text.topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ff.comm.text.topics.top <- ff.text.topics.top

ff.text.topics.plot <- ff.comm.text.topics.top %>% 
  ggplot(aes(reorder(term, beta), beta, fill = factor(topic))) + 
  geom_col(show.legend = FALSE) + 
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
  
```

```{r Topic Plot, echo=FALSE}

plot(ff.text.topics.plot)

```

While word clouds have limited utility in text analysis, this is a plot of the top terms sized according to frequency from trolls on election day. 

```{r Word Cloud, echo=FALSE, warning=FALSE,fig.width=14}


word.freq<-sort(rowSums(f.text.tdm.m), decreasing=T)
wordcloud(words=names(word.freq),
          freq = word.freq,
          random.order=F,
          max.words=150,
          colors=brewer.pal(8,"Dark2"))
title('Most frequent words during November 11th 2016',col.main='black',cex.main=1.5)

```

## Community Detection

Using cluster edge betweenness, high betweeness edges are removed sequentially untill the best partitioning of the network is achived. Each community had average betweeness centrality calculated to find the "community of information brokers" in the network. In this example, troll community 9 had the highest average betweeness centrality (2).

```{r Group Detection, include=FALSE}

# Community detection for subset F -----------------------------

# High-betweenness edges are removed sequentially (recalculating at each step) and the best partitioning of the network is selected.
set.seed(333)
f.ceb <- cluster_edge_betweenness(as.undirected(f.cent.rt.g))

##number of communities
f.clust.memb <- membership(f.ceb)
# assign community number back to user in the subset data frame 
f.clust.memb <- as.data.frame(as.numeric(f.clust.memb))
f.cent.df$membership <- f.clust.memb$`as.numeric(f.clust.memb)`

# By communitiy who has the highest betweeness? Find "community" of information broker trolls
# average bet by community membership for trolls
f.cent.df.troll <- f.cent.df %>% filter(troll == "troll")
f.comm.bet.mean <- aggregate( bet ~ membership, f.cent.df.troll, mean )
```

```{r ceb deg, echo=FALSE, warning=FALSE,fig.width=14}

head(f.comm.bet.mean[order(-f.comm.bet.mean$bet),], n=5)

```



```{r LDA Tuning, include=FALSE}
# GRoup 9 of the F subset has the highest betweenness
f.cent.df.comm.sub <- f.cent.df.troll %>% filter(membership == "9")
colnames(f)[colnames(f)=="user_key"] <- "user"
f.comm.text <- f.cent.df.comm.sub %>% left_join(f[,c("user","text")], by=c("user"))
f.comm.text <- f.comm.text$text

#c.comm.text

textfunction(f.comm.text)
f.comm.text.corp <- text.corp
f.comm.text.dtm.n <- text.dtm.n
f.comm.text.tdm.m <- text.corp.tdm.m


textfunction(f.comm.text)
f.comm.text.corp <- text.corp
f.comm.text.dtm.n <- text.dtm.n
f.comm.text.tdm.m <- text.corp.tdm.m

# Determine the "k" for LDA https://cran.r-project.org/web/packages/ldatuning/vignettes/topics.html

f.result <- FindTopicsNumber(
  f.comm.text.dtm.n,
  topics = seq(from = 2, to = 15, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 333),
  mc.cores = 6L,
  verbose = TRUE
)

# plot the number of topics found for community 9 of the F subset, 


```


```{r Betweeness Network, include=FALSE}

#graph from edge list
nodes.cent <- nodes.l %>% left_join(f.cent.df.troll[,c("user","membership")], by=c("user"))

# Non-trolls assigned membership to group 0
nodes.cent$membership[is.na(nodes.cent$membership)] <- 0

#Using links from before, with new nodes.cent membership df for troll community membership
cent.network.edge <- graph_from_data_frame(d=links, vertices = nodes.cent, directed=T) 

# customGreen0 = "#DeF7E9"
# customGreen = "#71CA97"
# customRed = "#ff7f7f"
# customYel = "#C9C271"

#Communities 11,5,9,7,6,8,12,22,17 have > 3 members "11","5","7","6","8","12","22","17"

V(cent.network.edge)$color <- ifelse(V(cent.network.edge)$membership == "9", "#ff4d4d", 
                                     ifelse(V(cent.network.edge)$membership %in% c("11","5","7","6","8","12","22","17"), brewer.pal(8,"Greens"), 
                                                                          "gray"))


```

Plot of the high betweenness community compared to the wider network graph, community 9 with highest average betweeness is show in red. Other detected communities containing more than 4 trolls shown in shades of green.

```{r, fig.height=10, fig.width=14, echo=FALSE, warning=FALSE}

set.seed(1434)
plot(cent.network.edge, 
     layout=layout.fruchterman.reingold,
     
     #Vertex/node
     vertex.size=((nodes.l$deg)/6),
     vertex.color=V(cent.network.edge)$color,
     
     # Node label
     vertex.label.cex=(((nodes.l$bet)/9)+0.3),
     vertex.label.color=V(cent.network.edge)$color,
     
     # Edge
     edge.width=.2,                      
     edge.arrow.size=0.02,                      
     edge.arrow.width=0.02,
     edge.curved=0.3,
     edge.color="gray"
     
)

```


## LDA Tuning

This community was then subset and their tweets were preprocessed as outlined above. On larger data sets LDA tuning has significant performance impacts. On this smaller subset of community 9, LDA tuning was performed to find an optimal K values (between K=2 and k=15) for further topic analysis.
The minimize function suggests a k=2, with the maximize function showing k=5. We'll setting on a K=3 for the remaining analysis.

```{r LDA tuning comm, echo=FALSE, warning=FALSE,fig.width=14}

FindTopicsNumber_plot(f.result)

```

```{r Community Topic, include=FALSE}

f.text.lda <- LDA(f.comm.text.dtm.n, k = 3, control = list(seed = 333))
f.text.topics <- tidy(f.text.lda, matrix = "beta")

f.text.topics.top <- f.text.topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

f.comm.text.topics.top <- f.text.topics.top

f.text.topics.plot <- f.comm.text.topics.top %>% 
  ggplot(aes(reorder(term, beta), beta, fill = factor(topic))) + 
  geom_col(show.legend = FALSE) + 
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

Information brokers community topic plot

```{r LDA tuning plot, echo=FALSE, warning=FALSE,fig.width=14}

plot(f.text.topics.plot)

```


```{r Community Topic Plot, eval=FALSE, include=FALSE}

#This doesn't work
 
# Learning new ways to fail ---------------------
# Attempt to graph the network according to their topics using k = 5
# Reset any global data frames for network function to the c subset,
# rename the user column which is used by the fucntion
 
c.node.topic <- c
colnames(c.node.topic)[colnames(c.node.topic)=="user"] <- "user_key"
 
plotfunction(c.node.topic)
 
edge.table <- rt.df
colnames(edge.table)[colnames(edge.table)=="rt.send"] <- "source"
colnames(edge.table)[colnames(edge.table)=="rt.rec"] <- "target"
 
# Reset any global data frames for text analysis to the c dataframe
textfunction(c.text)
c.node.text.corp <- text.corp
c.node.text.dtm.n <- text.dtm.n
c.node.text.tdm.m <- text.corp.tdm.m
 
 
c.node.dtm_lda <- LDA(c.node.text.dtm.n, k = 5, control = list(seed = 333))
c.node.topics_beta <- tidy(c.node.dtm_lda, matrix = "beta")

#pdf("18_C_subset_topics_network_attempt.pdf", width = 7, height = 7)
par(mfrow=c(1, 1),bg="#ffffff") #ffffff white 808484 grey
c.node.topics_beta %>%
 group_by(term) %>%
 top_n(1, beta) %>%
   group_by(topic) %>%
   top_n(50, beta) %>%
   acast(term ~ topic, value.var = "beta", fill = 0) %>%
   comparison.cloud(colors = brewer.pal(5, "Set1"))
# dev.off()
 
c.node.text.topics.top <- c.node.topics_beta %>%
   group_by(topic) %>%
   top_n(10, beta) %>%
   ungroup() %>%
   arrange(topic, -beta)
 
#c.node.text.topics.top
 
 
c.node.topics_gamma <- tidy(c.node.dtm_lda, matrix = "gamma") %>%
   arrange(desc(gamma))
 
c.node.user_topic <- c.node.topics_gamma %>%
   group_by(document) %>%
   top_n(1, gamma)
 
c.node.node_table <- data.frame(name = unique(c(as.character(edge.table$source), as.character(edge.table$target)))) %>%
   left_join(c.node.user_topic, by = c("name" = "document")) %>%
   unique()
 
 
c.node.node_table <- c.node.node_table[!duplicated(c.node.node_table$name), ]
#library(RColorBrewer)
 pal <- brewer.pal(5, "Set1")
 c.node.node_table$color = ifelse(c.node.node_table$topic == 1, pal[1],
                                  ifelse(c.node.node_table$topic == 2, pal[2],
                                         ifelse(c.node.node_table$topic == 3, pal[3],
                                                ifelse(c.node.node_table$topic == 4, pal[4], pal[5]))))
 
 
c.node.graph <- graph_from_data_frame(edge.table, directed = TRUE, vertices = c.node.node_table)
V(c.node.graph)$size <- ifelse(V(c.node.graph)$name == "ameliebaldwin", 4, 1)
V(c.node.graph)$label <- ifelse(V(c.node.graph)$name == "ameliebaldwin", "ameliebaldwin", NA)
 
node.graph <- graph_from_data_frame(edge.table, directed = TRUE)
layout <- layout_with_fr(node.graph)
V(node.graph)$color <- ifelse(V(node.graph)$name == "ameliebaldwin", "#377F97", "#4A9888")
V(node.graph)$size <- ifelse(V(node.graph)$name == "ameliebaldwin", 6, 1)
V(node.graph)$label <- ifelse(V(node.graph)$name == "ameliebaldwin", "ameliebaldwin", NA)
 
# pdf("twitter_net.pdf", width = 70, height = 80)
plot(node.graph,
      layout = layout,
      vertex.label = V(node.graph)$label,
      vertex.color = scales::alpha(V(node.graph)$color, alpha = 0.5), 
      vertex.size = V(node.graph)$size , 
      vertex.frame.color = "gray", 
      vertex.label.color = "black", 
      vertex.label.cex = 10,
      edge.arrow.size = 1)
# dev.off()
 
betweenness <- igraph::betweenness(node.graph, directed = TRUE)
betweenness[order(betweenness, decreasing = TRUE)]
edge_betweenness <- igraph::edge_betweenness(node.graph, directed = TRUE)
V(node.graph)$size <- ifelse(V(node.graph)$name == "ameliebaldwin", 10, betweenness * 0.000001)
 
# pdf("twitter_net_betweenness2.pdf", width = 70, height = 80)
plot(node.graph,
      layout = layout,
      vertex.label = V(node.graph)$label,
      vertex.color = scales::alpha(V(node.graph)$color, alpha = 0.5), 
      vertex.size = V(node.graph)$size, 
      vertex.frame.color = "gray", 
      vertex.label.color = "black", 
      vertex.label.cex = 6,
      edge.width = edge_betweenness * 0.0000001,
      edge.arrow.size = 1)
# dev.off()
 
 
 
#pdf("twitter_net_topics2.pdf", width = 2560, height = 1440)
# png("my_plot.png", 2560, 1440)
 plot(c.node.graph,
      layout = layout,
      vertex.label = V(node.graph)$label,
      vertex.color = scales::alpha(V(c.node.graph)$color, alpha = 0.4), 
      vertex.size = V(c.node.graph)$size , 
      vertex.frame.color = scales::alpha(V(c.node.graph)$color, alpha = 0.4), 
      vertex.label.color = scales::alpha("black", alpha = 1), 
      vertex.label.cex = 8,
      edge.color = scales::alpha("grey", alpha = 0.4),
      edge.arrow.size = 1)
legend("topright", legend = c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5"), pch = 19,
        col = pal, pt.cex = 10, cex = 8, bty = "n", ncol = 1,
        title = "Node color") 
# dev.off()
 

```
